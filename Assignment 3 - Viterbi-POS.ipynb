{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 498 - Assignment 3 - Viterbi.py\n",
    "### By: Alexander \"AJ\" Goldstein - uniquename: ajva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) train data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input 1: trainFile (string) - name of train data file\n",
    "# output 1: \n",
    "# output 2: \n",
    "\n",
    "def read_in_and_process_train_data(trainFile):\n",
    "    \n",
    "    # collect and store raw counts required for algorithm\n",
    "    allTags = [] # collect a list of all unique tags\n",
    "    tagFreqs_dict = {} # store corresponding frequency for each unique tag (KEY: tag, VALUE: count)\n",
    "    tagPairsFreq_dict = {} # store corresponding freq for each unique tag pair (KEY: concatenated tags, VALUE: count)\n",
    "    newWordTag_dict  = {} # store each new word as an individual dictionary of tag frequencies (KEY: tag, VALUE: count)\n",
    "    corpusWords_dict = {} # store all unique words (KEY: word, VALUE: newWordTag_dict)\n",
    "    \n",
    "    with open(trainFile) as data:\n",
    "        \n",
    "        # Step One: store all unique tags, tag pairs, and their corresponding counts\n",
    "        for line in data:\n",
    "            splitLine = line.split()\n",
    "            prevTag = 'BOSE'\n",
    "            for unit in splitLine:\n",
    "                word, slash, tag = unit.partition('/')\n",
    "                \n",
    "                # handle individual tag counts\n",
    "                if tag not in allTags:\n",
    "                    allTags.append(tag)\n",
    "                    tagFreqs_dict[tag] = 1\n",
    "                else:\n",
    "                    tagFreqs_dict[tag] += 1\n",
    "                \n",
    "                # handle tag pair counts\n",
    "                tagPair = prevTag + '_' + tag\n",
    "                \n",
    "                if tagPair not in tagPairsFreq_dict.keys():\n",
    "                    tagPairsFreq_dict[tagPair] = 1\n",
    "                else:\n",
    "                    tagPairsFreq_dict[tagPair] += 1\n",
    "                    \n",
    "                prevTag = tag\n",
    "        \n",
    "        # reset read file pointer\n",
    "        data.seek(0)\n",
    "        \n",
    "        # Step Two: store all unique words and their tag counts\n",
    "        for line in data:\n",
    "            splitLine = line.split()\n",
    "            for unit in splitLine:\n",
    "                word, slash, tag = unit.partition('/')\n",
    "                \n",
    "                if word not in corpusWords_dict.keys():\n",
    "                    corpusWords_dict[word] = dict.fromkeys(allTags, 0)\n",
    "                    corpusWords_dict[word][tag] += 1\n",
    "                    \n",
    "                else:\n",
    "                    corpusWords_dict[word][tag] += 1\n",
    "        \n",
    "        \n",
    "    return allTags, tagFreqs_dict, tagPairsFreq_dict, newWordTag_dict, corpusWords_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) test data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate out sentences from their tags\n",
    "def read_in_and_strip_test_data(testFile):\n",
    "    \n",
    "    # collect and store sentences (list of words) and their corresponding tags\n",
    "    sentToTags_dict = {}\n",
    "    \n",
    "    with open(testFile) as data:\n",
    "        \n",
    "        for line in data:\n",
    "            splitLine = line.split()\n",
    "            sentence = []\n",
    "            tags = []\n",
    "            for unit in splitLine:\n",
    "                word, slash, tag = unit.partition('/')\n",
    "                sentence.append(word)\n",
    "                tags.append(tag)\n",
    "            \n",
    "            sentence = tuple(sentence) # convert to tuple to store as dict key\n",
    "            sentToTags_dict[sentence] = tags\n",
    "        \n",
    "    testSentences = sentToTags_dict.keys()\n",
    "    testSolTags = sentToTags_dict.values()\n",
    "    \n",
    "    return testSentences, testSolTags, sentToTags_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_given_tag_prob(tag, prevTag, tagPairsFreq_dict):\n",
    "    \n",
    "    tagPair = prevTag + '_' + tag\n",
    "    \n",
    "    if tagPair in tagPairsFreq_dict:\n",
    "        tagPairCount = tagPairsFreq_dict[tagPair]\n",
    "        totalTagPairs = sum(tagPairsFreq_dict.values())\n",
    "        probability = float(tagPairCount/totalTagPairs)\n",
    "    else:\n",
    "        # if new tagPair, assume random\n",
    "        probability = float(1/len(tagPairsFreq_dict.keys()))\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_given_tag_prob(word, tag, corpusWords_dict, tagFreqs_dict):\n",
    "    \n",
    "    if word in corpusWords_dict:\n",
    "        wordTagFreq = corpusWords_dict[word][tag]\n",
    "        totalTagFreq = tagFreqs_dict[tag]\n",
    "        probability = float(wordTagFreq/totalTagFreq)\n",
    "        \n",
    "    else:\n",
    "        # if new word, assume random\n",
    "        probability = float(1/len(tagFreqs_dict.keys()))\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxFunction(probScores, allTags, currTag, prevWord, tagPairsFreq_dict):\n",
    "    maxProb = 0\n",
    "    maxTag = ''\n",
    "    for tag in allTags:\n",
    "        prob = probScores[(tag,prevWord)] * tag_given_tag_prob(currTag, tag, tagPairsFreq_dict)\n",
    "        if prob > maxProb:\n",
    "            maxProb = prob\n",
    "            maxTag = tag\n",
    "    \n",
    "    return maxProb, maxTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the tag with the maximum probability given a word\n",
    "def maxProbTag(probScores, allTags, word):\n",
    "    maxProb = 0\n",
    "    maxTag = ''\n",
    "    for tag in allTags:\n",
    "        prob = probScores[(tag,word)]\n",
    "        if prob >= maxProb:\n",
    "            maxProb = prob\n",
    "            maxTag = tag\n",
    "            \n",
    "    return maxTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_Viterbi_algo(testSentences, allTags, tagFreqs_dict, tagPairsFreq_dict, corpusWords_dict):\n",
    "    \n",
    "    # create storage for sentence tag predictions\n",
    "    sentToTagPreds_dict = {}\n",
    "    \n",
    "    # create and set up storage for tag-word combinations\n",
    "    tagWordTuples = []\n",
    "    allWords = corpusWords_dict.keys()\n",
    "    for word in allWords:\n",
    "        for tag in allTags:\n",
    "            tagWordTuples.append((tag, word))\n",
    "    \n",
    "    # iterate through every test sentence\n",
    "    for num, sentence in enumerate(testSentences):\n",
    "\n",
    "        # 0) create storage for this sentence's predictions, probability scores, and back tag pointer\n",
    "        tagPreds = ['']*len(sentence)\n",
    "        probScores = dict.fromkeys(tagWordTuples, 0)\n",
    "        backTagPtr = dict.fromkeys(tagWordTuples, 0)\n",
    "\n",
    "        # 1) Initialization\n",
    "        for tag in allTags:\n",
    "            wordTagProb = word_given_tag_prob(sentence[0], tag, corpusWords_dict, tagFreqs_dict)\n",
    "            tagTagProb = tag_given_tag_prob(tag,'BOSE', tagPairsFreq_dict)\n",
    "            probScores[(tag, sentence[0])] =  wordTagProb * tagTagProb\n",
    "            backTagPtr[(tag, sentence[0])] = None\n",
    "\n",
    "        # 2) Iteration\n",
    "        prevWord = sentence[0]\n",
    "        for word in sentence[1:]:\n",
    "            for tag in allTags:\n",
    "                maxProb, maxPrevTag = maxFunction(probScores, allTags, tag, prevWord, tagPairsFreq_dict)\n",
    "                wordTagProb = word_given_tag_prob(word, tag, corpusWords_dict, tagFreqs_dict)\n",
    "                probScores[(tag, word)] = wordTagProb * maxProb\n",
    "                backTagPtr[(tag, word)] = maxPrevTag\n",
    "            prevWord = word\n",
    "        \n",
    "        # 3) Sequence Identification\n",
    "        \n",
    "        # 3.1) find most probable tag for last word\n",
    "        lastWord = sentence[len(sentence)-1]\n",
    "        if lastWord in allWords:\n",
    "            lastTagPred = maxProbTag(probScores, allTags, lastWord)\n",
    "        else:\n",
    "            lastTagPred = random.choice(allTags)\n",
    "        tagPreds[len(sentence)-1] = lastTagPred # store tag\n",
    "            \n",
    "        # 3.2) loop through all words (except last) in reverse order\n",
    "        for index, word in enumerate(reversed(sentence[:-1])):\n",
    "            \n",
    "            # get nextWord in sentence to use for backTagPtr\n",
    "            nextWord = sentence[len(sentence)-1-index]\n",
    "            \n",
    "            # if last word, set tag to above\n",
    "            if nextWord == lastWord:\n",
    "                nextTagPred = lastTagPred\n",
    "            # if nextWord was in training data, find it's most probable tag\n",
    "            elif nextWord in allWords:\n",
    "                nextTagPred = maxProbTag(probScores, allTags, nextWord)\n",
    "            else:\n",
    "                # otherwise, randomize the tag\n",
    "                nextTagPred = random.choice(allTags)\n",
    "            \n",
    "            # store tag for previous word using next word\n",
    "            tagPreds[len(sentence)-2-index] = backTagPtr[(nextTagPred, nextWord)]\n",
    "\n",
    "        # 3.3) store predictions for this sentence\n",
    "        sentence = tuple(sentence)\n",
    "        sentToTagPreds_dict[sentence] = tagPreds\n",
    "        \n",
    "    return sentToTagPreds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_simple_baseline(testSentences, allTags, corpusWords_dict):\n",
    "    \n",
    "    # define all words from training set\n",
    "    allWords = corpusWords_dict.keys()\n",
    "    \n",
    "    # create storage for sentence tag predictions\n",
    "    sentToTagPreds_dict = {}\n",
    "    \n",
    "    # iterate through every test sentence\n",
    "    for sentNum, sentence in enumerate(testSentences):\n",
    "        # create storage\n",
    "        tagPreds = ['']*len(sentence)\n",
    "        for wordNum, word in enumerate(sentence):\n",
    "            \n",
    "            # generate tag prediction\n",
    "            if word in allWords:\n",
    "                wordTag_dict = corpusWords_dict[word]\n",
    "                maxTag = max(wordTag_dict, key=wordTag_dict.get)\n",
    "            else:\n",
    "                maxTag = random.choice(allTags)\n",
    "            \n",
    "            # store predicted tag\n",
    "            tagPreds[wordNum] = maxTag\n",
    "        \n",
    "        # store predicted sentence of tags\n",
    "        sentence = tuple(sentence)\n",
    "        sentToTagPreds_dict[sentence] = tagPreds\n",
    "    \n",
    "    return sentToTagPreds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(sentToTagPreds_dict, sentToTagTruth_dict):\n",
    "    \n",
    "    correctTagCount = 0\n",
    "    totalTags = 0\n",
    "    incorrectPreds = []\n",
    "    \n",
    "    for sentence in sentToTagPreds_dict.keys():\n",
    "        truthSentTags = sentToTagTruth_dict[sentence]\n",
    "        predSentTags = sentToTagPreds_dict[sentence]\n",
    "        for wordIndex, (truthTag, predTag) in enumerate(zip(truthSentTags, predSentTags)):\n",
    "            totalTags += 1\n",
    "            if predTag == truthTag:\n",
    "                correctTagCount += 1\n",
    "            else:\n",
    "                # store word, incorrect prediction, and truth tag\n",
    "                incorrectPreds.append([sentence[wordIndex], predTag, truthTag])\n",
    "    \n",
    "    accuracy = float(correctTagCount/totalTags)\n",
    "    \n",
    "    return accuracy, incorrectPreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Predictions Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predictions_output(sentToTagPreds_dict):\n",
    "    \n",
    "    output = \"POS.test.out\"\n",
    "    f = open(output, \"w\")\n",
    "\n",
    "    for sentence, sentTags in sentToTagPreds_dict.items():\n",
    "        for word, tag in zip(sentence, sentTags):\n",
    "            f.write(word + '/' + tag + ' ')\n",
    "        f.write('\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_incorrect_predictions(incorrectPreds):\n",
    "    print('incorrect predictions:' + '\\n')\n",
    "    for pred in incorrectPreds[:10]:\n",
    "        print('word: ' + pred[0] + ' ')\n",
    "        print('prediction: ' + pred[1] + ' ')\n",
    "        print('truth: ' + pred[2] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read in file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SCRIPT NOTE: switch out arguments\n",
    "# read in file names\n",
    "trainFile = \"POS.test\"\n",
    "testFile = \"POS.train\"\n",
    "\n",
    "#trainFile = sys.argv[1]\n",
    "#testFile = sys.argv[2]\n",
    "\n",
    "largeTrain = \"POS.train.large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Read in and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "allTags, tagFreqs_dict, tagPairsFreq_dict, newWordTag_dict, corpusWords_dict = read_in_and_process_train_data(trainFile)\n",
    "\n",
    "# test data\n",
    "testSentences, testSolTags, sentToTagsTruth_dict = read_in_and_strip_test_data(testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Run Viterbi algorithm on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run algorithm\n",
    "sentToTagPreds_dict = run_Viterbi_algo(testSentences[:50], allTags, tagFreqs_dict, tagPairsFreq_dict, corpusWords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Calculate Viterbi accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Viterbi model accuracy:', 0.7231245166279969)\n",
      "incorrect predictions:\n",
      "\n",
      "word: EST \n",
      "prediction: NN \n",
      "truth: NP\n",
      "\n",
      "word: Nov. \n",
      "prediction: DT \n",
      "truth: NP\n",
      "\n",
      "word: extended \n",
      "prediction: DT \n",
      "truth: VBN\n",
      "\n",
      "word: year \n",
      "prediction: NP \n",
      "truth: NN\n",
      "\n",
      "word: , \n",
      "prediction: CC \n",
      "truth: ,\n",
      "\n",
      "word: , \n",
      "prediction: IN \n",
      "truth: ,\n",
      "\n",
      "word: New \n",
      "prediction: JJ \n",
      "truth: NP\n",
      "\n",
      "word: York-based \n",
      "prediction: JJ \n",
      "truth: NP\n",
      "\n",
      "word: of \n",
      "prediction: CC \n",
      "truth: IN\n",
      "\n",
      "word: bought \n",
      "prediction: VBN \n",
      "truth: VBD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare sentToTagPreds_dict to sentToTagsTruth_dict\n",
    "accuracy, incorrectPreds = calculate_accuracy(sentToTagPreds_dict, sentToTagsTruth_dict)\n",
    "print('Viterbi model accuracy:', accuracy)\n",
    "\n",
    "# print incorrect predictions\n",
    "print_incorrect_predictions(incorrectPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Write predictions output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predictions output file\n",
    "write_predictions_output(sentToTagPreds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Run simple baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baselinePreds = run_simple_baseline(testSentences[:50], allTags, corpusWords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Calculate baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baseline model accuracy:', 0.679814385150812)\n",
      "incorrect predictions:\n",
      "\n",
      "word: offer \n",
      "prediction: VB \n",
      "truth: NN\n",
      "\n",
      "word: EST \n",
      "prediction: VBN \n",
      "truth: NP\n",
      "\n",
      "word: Nov. \n",
      "prediction: EX \n",
      "truth: NP\n",
      "\n",
      "word: extended \n",
      "prediction: ) \n",
      "truth: VBN\n",
      "\n",
      "word: York-based \n",
      "prediction: JJ \n",
      "truth: NP\n",
      "\n",
      "word: arm \n",
      "prediction: ABC/NP \n",
      "truth: NN\n",
      "\n",
      "word: bought \n",
      "prediction: VBN \n",
      "truth: VBD\n",
      "\n",
      "word: controlling \n",
      "prediction: VBN \n",
      "truth: VBG\n",
      "\n",
      "word: glass \n",
      "prediction: '' \n",
      "truth: NN\n",
      "\n",
      "word: venture \n",
      "prediction: MD \n",
      "truth: NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare sentToTagPreds_dict to sentToTagsTruth_dict\n",
    "accuracy, incorrectPreds = calculate_accuracy(baselinePreds, sentToTagsTruth_dict)\n",
    "print('Baseline model accuracy:', accuracy)\n",
    "\n",
    "# print incorrect predictions\n",
    "print_incorrect_predictions(incorrectPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Process & test using large training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "allTags, tagFreqs_dict, tagPairsFreq_dict, newWordTag_dict, corpusWords_dict = read_in_and_process_train_data(largeTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run Viterbi algorithm\n",
    "sentToTagPreds_dict = run_Viterbi_algo(testSentences[:50], allTags, tagFreqs_dict, tagPairsFreq_dict, corpusWords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Viterbi model accuracy (LARGE TRAIN):', 0.7215777262180975)\n"
     ]
    }
   ],
   "source": [
    "# calculate Viterbi accuracy\n",
    "accuracy, incorrectPreds = calculate_accuracy(sentToTagPreds_dict, sentToTagsTruth_dict)\n",
    "print('Viterbi model accuracy (LARGE TRAIN):', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run Baseline model\n",
    "baselinePreds = run_simple_baseline(testSentences[:50], allTags, corpusWords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baseline model accuracy (LARGE TRAIN):', 0.6767208043310131)\n"
     ]
    }
   ],
   "source": [
    "# calculate baseline accuracy\n",
    "accuracy, incorrectPreds = calculate_accuracy(baselinePreds, sentToTagsTruth_dict)\n",
    "print('Baseline model accuracy (LARGE TRAIN):', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
